{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c240e82b",
   "metadata": {},
   "source": [
    "# Retail Analytics Copilot - Self-Contained Run\n",
    "This notebook creates all necessary project files on the Colab instance and runs the agent using the Tesla T4 GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382fdac",
   "metadata": {},
   "source": [
    "## 1. Setup Environment & Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c002932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "Requirement already satisfied: dspy-ai in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (8.3.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (13.9.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
      "Requirement already satisfied: dspy>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from dspy-ai) (3.0.4)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.47)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: backoff>=2.2 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.2.1)\n",
      "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.8.1)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2025.11.3)\n",
      "Requirement already satisfied: orjson>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (3.11.4)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.32.4)\n",
      "Requirement already satisfied: optuna>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.6.0)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (0.1.6)\n",
      "Requirement already satisfied: litellm>=1.64.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (1.80.5)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (0.54.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.11.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (6.2.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (3.1.2)\n",
      "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (11.3.0)\n",
      "Requirement already satisfied: gepa==0.0.17 in /usr/local/lib/python3.12/dist-packages (from gepa[dspy]==0.0.17->dspy>=3.0.4->dspy-ai) (0.0.17)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.4->dspy-ai) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.4->dspy-ai) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.13.2)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.14.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (4.25.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.22.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.4->dspy-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.4->dspy-ai) (0.12.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (2.0.44)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy>=3.0.4->dspy-ai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy>=3.0.4->dspy-ai) (2.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.22.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (1.3.10)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.29.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!pip install dspy-ai langgraph langchain-core pydantic click rich numpy pandas scikit-learn rank-bm25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8fbebb",
   "metadata": {},
   "source": [
    "## 2. Start Ollama & Pull Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7eb7a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ollama server...\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start Ollama server in the background\n",
    "subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"Starting Ollama server...\")\n",
    "time.sleep(5)\n",
    "\n",
    "!ollama pull phi3.5:3.8b-mini-instruct-q4_K_M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0b387",
   "metadata": {},
   "source": [
    "## 3. Create Project Files (Scaffolding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46a81a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"agent/rag\", exist_ok=True)\n",
    "os.makedirs(\"agent/tools\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "# Create __init__.py files\n",
    "for path in [\"agent/__init__.py\", \"agent/rag/__init__.py\", \"agent/tools/__init__.py\"]:\n",
    "    with open(path, \"w\") as f: pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6bacac",
   "metadata": {},
   "source": [
    "## 4. Create Data & Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c39cf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Database\n",
    "import urllib.request\n",
    "if not os.path.exists(\"data/northwind.sqlite\"):\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/jpwhite3/northwind-SQLite3/main/dist/northwind.db', 'data/northwind.sqlite')\n",
    "\n",
    "# Create Docs\n",
    "files = {\n",
    "    \"docs/marketing_calendar.md\": \"\"\"# Northwind Marketing Calendar (1997)\n",
    "## Summer Beverages 1997\n",
    "- Dates: 1997-06-01 to 1997-06-30\n",
    "- Notes: Focus on Beverages and Condiments.\n",
    "## Winter Classics 1997\n",
    "- Dates: 1997-12-01 to 1997-12-31\n",
    "- Notes: Push Dairy Products and Confections for holiday gifting.\"\"\",\n",
    "    \n",
    "    \"docs/kpi_definitions.md\": \"\"\"# KPI Definitions\n",
    "## Average Order Value (AOV)\n",
    "- AOV = SUM(UnitPrice * Quantity * (1 - Discount)) / COUNT(DISTINCT OrderID)\n",
    "## Gross Margin\n",
    "- GM = SUM((UnitPrice - CostOfGoods) * Quantity * (1 - Discount))\n",
    "- If cost is missing, approximate with category-level average (document your approach).\"\"\",\n",
    "    \n",
    "    \"docs/catalog.md\": \"\"\"# Catalog Snapshot\n",
    "- Categories include Beverages, Condiments, Confections, Dairy Products, Grains/Cereals, Meat/Poultry, Produce, Seafood.\n",
    "- Products map to categories as in the Northwind DB.\"\"\",\n",
    "    \n",
    "    \"docs/product_policy.md\": \"\"\"# Returns & Policy\n",
    "- Perishables (Produce, Seafood, Dairy): 3 7 days.\n",
    "- Beverages unopened: 14 days; opened: no returns.\n",
    "- Non-perishables: 30 days.\"\"\"\n",
    "}\n",
    "\n",
    "for path, content in files.items():\n",
    "    with open(path, \"w\") as f: f.write(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf610b69",
   "metadata": {},
   "source": [
    "## 5. Create Code Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d498925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent/tools/sqlite_tool.py\n",
    "with open(\"agent/tools/sqlite_tool.py\", \"w\") as f:\n",
    "    f.write('''import sqlite3\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "class SQLiteTool:\n",
    "    def __init__(self, db_path: str = \"data/northwind.sqlite\"):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def _get_connection(self):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        return conn\n",
    "\n",
    "    def list_tables(self) -> List[str]:\n",
    "        conn = self._get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        return tables\n",
    "\n",
    "    def get_schema(self, table_names: Optional[List[str]] = None) -> str:\n",
    "        conn = self._get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        if not table_names:\n",
    "            table_names = self.list_tables()\n",
    "        schema_str = []\n",
    "        for table in table_names:\n",
    "            cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name=?\", (table,))\n",
    "            res = cursor.fetchone()\n",
    "            if res:\n",
    "                schema_str.append(res[0] + \";\")\n",
    "        conn.close()\n",
    "        sep = chr(10) + chr(10)\n",
    "        return sep.join(schema_str)\n",
    "\n",
    "    def execute_sql(self, query: str) -> Tuple[List[Dict[str, Any]], List[str], Optional[str]]:\n",
    "        conn = self._get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            if query.strip().lower().startswith(\"select\"):\n",
    "                results = [dict(row) for row in cursor.fetchall()]\n",
    "                columns = [description[0] for description in cursor.description] if cursor.description else []\n",
    "                conn.close()\n",
    "                return results, columns, None\n",
    "            else:\n",
    "                conn.commit()\n",
    "                conn.close()\n",
    "                return [], [], None\n",
    "        except Exception as e:\n",
    "            conn.close()\n",
    "            return [], [], str(e)\n",
    "''')\n",
    "\n",
    "# agent/rag/retrieval.py\n",
    "with open(\"agent/rag/retrieval.py\", \"w\") as f:\n",
    "    f.write('''import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "class SimpleRetriever:\n",
    "    def __init__(self, docs_dir: str = \"docs\"):\n",
    "        self.docs_dir = docs_dir\n",
    "        self.chunks = []\n",
    "        self.bm25 = None\n",
    "        self._load_and_index()\n",
    "\n",
    "    def _load_and_index(self):\n",
    "        self.chunks = []\n",
    "        tokenized_corpus = []\n",
    "        for filename in os.listdir(self.docs_dir):\n",
    "            if filename.endswith(\".md\"):\n",
    "                filepath = os.path.join(self.docs_dir, filename)\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                newline = chr(10)\n",
    "                pattern = newline + r'#{1,3} |' + newline + newline\n",
    "                raw_chunks = re.split(pattern, content)\n",
    "                for i, text in enumerate(raw_chunks):\n",
    "                    if text.strip():\n",
    "                        chunk_id = f\"{filename}::chunk{i}\"\n",
    "                        self.chunks.append({\n",
    "                            \"id\": chunk_id,\n",
    "                            \"content\": text.strip(),\n",
    "                            \"source\": filename\n",
    "                        })\n",
    "                        tokenized_corpus.append(text.lower().split())\n",
    "        if tokenized_corpus:\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        if not self.bm25:\n",
    "            return []\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_n_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "        results = []\n",
    "        for idx in top_n_indices:\n",
    "            results.append({\n",
    "                **self.chunks[idx],\n",
    "                \"score\": scores[idx]\n",
    "            })\n",
    "        return results\n",
    "''')\n",
    "\n",
    "# agent/dspy_signatures.py\n",
    "with open(\"agent/dspy_signatures.py\", \"w\") as f:\n",
    "    f.write('''import dspy\n",
    "from typing import List, Optional\n",
    "\n",
    "class RouterSignature(dspy.Signature):\n",
    "    question = dspy.InputField(desc=\"The user's retail analytics question\")\n",
    "    classification = dspy.OutputField(desc=\"One of: 'rag', 'sql', 'hybrid'\")\n",
    "\n",
    "class Router(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(RouterSignature)\n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)\n",
    "\n",
    "class PlannerSignature(dspy.Signature):\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField(desc=\"Relevant chunks from documentation\")\n",
    "    date_range_start = dspy.OutputField(desc=\"Start date YYYY-MM-DD or None\")\n",
    "    date_range_end = dspy.OutputField(desc=\"End date YYYY-MM-DD or None\")\n",
    "    kpi_formula = dspy.OutputField(desc=\"Relevant KPI formula text or None\")\n",
    "    entities = dspy.OutputField(desc=\"List of relevant entities (products, categories, customers)\")\n",
    "\n",
    "class Planner(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(PlannerSignature)\n",
    "    def forward(self, question, context):\n",
    "        return self.prog(question=question, context=context)\n",
    "\n",
    "class TextToSQLSignature(dspy.Signature):\n",
    "    question = dspy.InputField()\n",
    "    schema = dspy.InputField(desc=\"SQLite CREATE TABLE statements\")\n",
    "    constraints = dspy.InputField(desc=\"Specific constraints (dates, formulas) from docs\")\n",
    "    sql_query = dspy.OutputField(desc=\"Valid SQLite query\")\n",
    "    explanation = dspy.OutputField(desc=\"Brief explanation of the logic\")\n",
    "\n",
    "class TextToSQL(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(TextToSQLSignature)\n",
    "    def forward(self, question, schema, constraints):\n",
    "        return self.prog(question=question, schema=schema, constraints=constraints)\n",
    "\n",
    "class SynthesizerSignature(dspy.Signature):\n",
    "    question = dspy.InputField()\n",
    "    sql_query = dspy.InputField(desc=\"Executed SQL query (if any)\")\n",
    "    sql_result = dspy.InputField(desc=\"Result rows from SQL execution\")\n",
    "    retrieved_context = dspy.InputField(desc=\"Text chunks from documentation\")\n",
    "    format_hint = dspy.InputField(desc=\"Expected output format (e.g., 'int', 'float', 'json')\")\n",
    "    final_answer = dspy.OutputField(desc=\"The typed answer matching format_hint exactly\")\n",
    "    citations = dspy.OutputField(desc=\"List of strings: table names and doc chunk IDs used\")\n",
    "\n",
    "class Synthesizer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(SynthesizerSignature)\n",
    "    def forward(self, question, sql_query, sql_result, retrieved_context, format_hint):\n",
    "        return self.prog(question=question, sql_query=sql_query, sql_result=sql_result, retrieved_context=retrieved_context, format_hint=format_hint)\n",
    "''')\n",
    "\n",
    "# agent/graph_hybrid.py\n",
    "with open(\"agent/graph_hybrid.py\", \"w\") as f:\n",
    "    f.write('''import dspy\n",
    "import sqlite3\n",
    "import os\n",
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional, Union\n",
    "from langgraph.graph import StateGraph, END\n",
    "from agent.dspy_signatures import Router, Planner, TextToSQL, Synthesizer\n",
    "from agent.rag.retrieval import SimpleRetriever\n",
    "from agent.tools.sqlite_tool import SQLiteTool\n",
    "\n",
    "def setup_dspy():\n",
    "    lm = dspy.LM(model=\"ollama_chat/phi3.5:3.8b-mini-instruct-q4_K_M\", api_base=\"http://localhost:11434\", api_key=\"\")\n",
    "    dspy.settings.configure(lm=lm)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    format_hint: str\n",
    "    classification: Optional[str]\n",
    "    retrieved_docs: List[Dict]\n",
    "    constraints: Dict[str, Any]\n",
    "    schema: str\n",
    "    sql_query: Optional[str]\n",
    "    sql_results: Optional[List[Dict]]\n",
    "    sql_columns: Optional[List[str]]\n",
    "    sql_error: Optional[str]\n",
    "    final_answer: Any\n",
    "    citations: List[str]\n",
    "    repair_count: int\n",
    "    repair_feedback: Optional[str]\n",
    "\n",
    "def router_node(state: AgentState):\n",
    "    router = Router()\n",
    "    pred = router(question=state[\"question\"])\n",
    "    cls = pred.classification.lower().strip()\n",
    "    if \"hybrid\" in cls: return {\"classification\": \"hybrid\"}\n",
    "    if \"sql\" in cls: return {\"classification\": \"sql\"}\n",
    "    return {\"classification\": \"rag\"}\n",
    "\n",
    "def retriever_node(state: AgentState):\n",
    "    retriever = SimpleRetriever()\n",
    "    docs = retriever.search(state[\"question\"], k=3)\n",
    "    return {\"retrieved_docs\": docs}\n",
    "\n",
    "def planner_node(state: AgentState):\n",
    "    planner = Planner()\n",
    "    sep = chr(10) + chr(10)\n",
    "    context_str = sep.join([d['content'] for d in state[\"retrieved_docs\"]])\n",
    "    try:\n",
    "        pred = planner(question=state[\"question\"], context=context_str)\n",
    "        constraints = {\n",
    "            \"date_range_start\": getattr(pred, 'date_range_start', None),\n",
    "            \"date_range_end\": getattr(pred, 'date_range_end', None),\n",
    "            \"kpi_formula\": getattr(pred, 'kpi_formula', None),\n",
    "            \"entities\": getattr(pred, 'entities', [])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Planner parsing error: {e}, using default constraints\")\n",
    "        constraints = {\"date_range_start\": None, \"date_range_end\": None, \"kpi_formula\": None, \"entities\": []}\n",
    "    return {\"constraints\": constraints}\n",
    "\n",
    "def sql_generator_node(state: AgentState):\n",
    "    tool = SQLiteTool()\n",
    "    schema = tool.get_schema()\n",
    "    constraints_str = str(state.get(\"constraints\", \"None\"))\n",
    "    if state.get(\"repair_feedback\"):\n",
    "        constraints_str += chr(10) + f\"PREVIOUS ERROR: {state['repair_feedback']}. FIX THIS.\"\n",
    "    generator = TextToSQL()\n",
    "    opt_path = os.path.join(os.getcwd(), \"agent\", \"optimized_sql_module.json\")\n",
    "    if os.path.exists(opt_path):\n",
    "        try:\n",
    "            generator.load(opt_path)\n",
    "            print(f\"Loaded optimized SQL module from {opt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load optimized module: {e}\")\n",
    "    pred = generator(question=state[\"question\"], schema=schema, constraints=constraints_str)\n",
    "    raw_sql = pred.sql_query.strip()\n",
    "    clean_sql = raw_sql.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "    return {\"sql_query\": clean_sql, \"schema\": schema}\n",
    "\n",
    "def executor_node(state: AgentState):\n",
    "    tool = SQLiteTool()\n",
    "    results, cols, error = tool.execute_sql(state[\"sql_query\"])\n",
    "    return {\"sql_results\": results, \"sql_columns\": cols, \"sql_error\": error}\n",
    "\n",
    "def repair_check_node(state: AgentState):\n",
    "    if state[\"sql_error\"] and state[\"repair_count\"] < 2:\n",
    "        return \"repair\"\n",
    "    return \"synthesize\"\n",
    "\n",
    "def repair_node(state: AgentState):\n",
    "    return {\"repair_count\": state[\"repair_count\"] + 1, \"repair_feedback\": state[\"sql_error\"] or \"Invalid format or empty result\"}\n",
    "\n",
    "def synthesizer_node(state: AgentState):\n",
    "    synthesizer = Synthesizer()\n",
    "    sep = chr(10) + chr(10)\n",
    "    context_str = sep.join([d['content'] for d in state.get(\"retrieved_docs\", [])])\n",
    "    pred = synthesizer(question=state[\"question\"], sql_query=state.get(\"sql_query\", \"\"), sql_result=str(state.get(\"sql_results\", [])), retrieved_context=context_str, format_hint=state[\"format_hint\"])\n",
    "    return {\"final_answer\": pred.final_answer, \"citations\": pred.citations}\n",
    "\n",
    "def build_graph():\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"router\", router_node)\n",
    "    workflow.add_node(\"retriever\", retriever_node)\n",
    "    workflow.add_node(\"planner\", planner_node)\n",
    "    workflow.add_node(\"sql_generator\", sql_generator_node)\n",
    "    workflow.add_node(\"executor\", executor_node)\n",
    "    workflow.add_node(\"repair\", repair_node)\n",
    "    workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "    workflow.set_entry_point(\"router\")\n",
    "    workflow.add_conditional_edges(\"router\", lambda x: x[\"classification\"], {\"rag\": \"retriever\", \"sql\": \"sql_generator\", \"hybrid\": \"retriever\"})\n",
    "    workflow.add_conditional_edges(\"retriever\", lambda x: x[\"classification\"], {\"rag\": \"synthesizer\", \"hybrid\": \"planner\"})\n",
    "    workflow.add_edge(\"planner\", \"sql_generator\")\n",
    "    workflow.add_edge(\"sql_generator\", \"executor\")\n",
    "    workflow.add_conditional_edges(\"executor\", repair_check_node, {\"repair\": \"repair\", \"synthesize\": \"synthesizer\"})\n",
    "    workflow.add_edge(\"repair\", \"sql_generator\")\n",
    "    workflow.add_edge(\"synthesizer\", END)\n",
    "    return workflow.compile()\n",
    "''')\n",
    "\n",
    "# agent/optimize_sql.py\n",
    "with open(\"agent/optimize_sql.py\", \"w\") as f:\n",
    "    f.write('''import sys, os, dspy\n",
    "sys.path.append(os.getcwd())\n",
    "from agent.dspy_signatures import TextToSQL\n",
    "from agent.tools.sqlite_tool import SQLiteTool\n",
    "\n",
    "def sql_metric(example, pred, trace=None):\n",
    "    tool = SQLiteTool()\n",
    "    sql = pred.sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "    results, cols, error = tool.execute_sql(sql)\n",
    "    return error is None\n",
    "\n",
    "def optimize_sql_module():\n",
    "    lm = dspy.LM(model=\"ollama_chat/phi3.5:3.8b-mini-instruct-q4_K_M\", api_base=\"http://localhost:11434\", api_key=\"\")\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    tool = SQLiteTool()\n",
    "    schema = tool.get_schema([\"Orders\", \"Order Details\", \"Products\"])\n",
    "    train_examples = [\n",
    "        dspy.Example(question=\"How many products are there?\", schema=schema, constraints=\"None\", sql_query=\"SELECT COUNT(*) FROM Products;\").with_inputs(\"question\", \"schema\", \"constraints\"),\n",
    "        dspy.Example(question=\"What is the total revenue from Order 10248?\", schema=schema, constraints=\"Revenue = UnitPrice * Quantity * (1-Discount)\", sql_query='SELECT SUM(UnitPrice * Quantity * (1 - Discount)) FROM \"Order Details\" WHERE OrderID = 10248;').with_inputs(\"question\", \"schema\", \"constraints\"),\n",
    "        dspy.Example(question=\"List all products in CategoryID 1.\", schema=schema, constraints=\"None\", sql_query=\"SELECT ProductName FROM Products WHERE CategoryID = 1;\").with_inputs(\"question\", \"schema\", \"constraints\")\n",
    "    ]\n",
    "    print(\"Starting optimization with BootstrapFewShot...\")\n",
    "    from dspy.teleprompt import BootstrapFewShot\n",
    "    teleprompter = BootstrapFewShot(metric=sql_metric, max_bootstrapped_demos=2)\n",
    "    compiled_sql = teleprompter.compile(TextToSQL(), trainset=train_examples)\n",
    "    output_path = os.path.join(\"agent\", \"optimized_sql_module.json\")\n",
    "    compiled_sql.save(output_path)\n",
    "    print(f\"Optimization complete! Saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimize_sql_module()\n",
    "''')\n",
    "\n",
    "# sample_questions_hybrid_eval.jsonl\n",
    "with open(\"sample_questions_hybrid_eval.jsonl\", \"w\") as f:\n",
    "    f.write('''{\"id\":\"rag_policy_beverages_return_days\",\"question\":\"According to the product policy, what is the return window (days) for unopened Beverages? Return an integer.\",\"format_hint\":\"int\"}\n",
    "{\"id\":\"hybrid_top_category_qty_summer_1997\",\"question\":\"During 'Summer Beverages 1997' as defined in the marketing calendar, which product category had the highest total quantity sold? Return {category:str, quantity:int} .\",\"format_hint\":\"{category:str, quantity:int}\"}\n",
    "{\"id\":\"hybrid_aov_winter_1997\",\"question\":\"Using the AOV definition from the KPI docs, what was the Average Order Value during 'Winter Classics 1997'? Return a float rounded to 2 decimals.\",\"format_hint\":\"float\"}\n",
    "{\"id\":\"sql_top3_products_by_revenue_alltime\",\"question\":\"Top 3 products by total revenue all-time. Revenue uses Order Details: SUM(UnitPrice*Quantity*(1-Discount)). Return list[{product:str, revenue:float}] .\",\"format_hint\":\"list[{product:str, revenue:float}]\"}\n",
    "{\"id\":\"hybrid_revenue_beverages_summer_1997\",\"question\":\"Total revenue from the 'Beverages' category during 'Summer Beverages 1997' dates. Return a float rounded to 2 decimals.\",\"format_hint\":\"float\"}\n",
    "{\"id\":\"hybrid_best_customer_margin_1997\",\"question\":\"Per the KPI definition of gross margin, who was the top customer by gross margin in 1997? Assume CostOfGoods is approximated by 70% of UnitPrice if not available. Return {customer:str, margin:float} .\",\"format_hint\":\"{customer:str, margin:float}\"}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37e483",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff74df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/dspy/signatures/signature.py:154: UserWarning: Field name \"schema\" in \"TextToSQLSignature\" shadows an attribute in parent \"Signature\"\n",
      "  cls = super().__new__(mcs, signature_name, bases, namespace, **kwargs)\n",
      "Starting optimization with BootstrapFewShot...\n",
      "/usr/local/lib/python3.12/dist-packages/dspy/signatures/signature.py:154: UserWarning: Field name \"schema\" in \"StringSignature\" shadows an attribute in parent \"Signature\"\n",
      "  cls = super().__new__(mcs, signature_name, bases, namespace, **kwargs)\n",
      "100% 3/3 [00:00<00:00, 16.72it/s]\n",
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Optimization complete! Saved to agent/optimized_sql_module.json\n"
     ]
    }
   ],
   "source": [
    "!python agent/optimize_sql.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2e5da",
   "metadata": {},
   "source": [
    "## 7. Run Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0056cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded optimized SQL module from /content/agent/optimized_sql_module.json\n",
      "Error processing hybrid_top_category_qty_summer_1997: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {\n",
      " \"reasoning\": \"The provided text does not contain enough information to determine which product category had the highest total sales for 'Summer Beverages' during that particular year (1997). To provide an accurate answer, specific data or statistics regarding monthly/quarterly sales of different categories within beverages sold would need to be provided. Without this detailed dataset information on how many units were sold in each category throughout the summer season of 1997 and a comparison among these quantities for 'Summer Beverages', it's impossible to discern which product had been outperforming others or determine if there was any significant shift from one beverage type over another. Therefore, an accurate classification is not feasible with only the given information.\"\n",
      " \n",
      "  \n",
      "   \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, classification] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:00<00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hybrid_aov_winter_1997: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {\n",
      " \"reasoning\": \"The instruction provided does not present an actual question but rather asks for information or data analysis based on AOV (Average Order Value) definition from KPIs. The task requires extracting the average value of a financial metric, which can be found in sales and business analytics context typically through analyzing numerical datasets related to orders during that specific period 'Winter Classics 1997'. Since there are no data provided within this instruction for an actual calculation or analysis (no numbers given), it's not possible to generate a real-world answer. However, if you were seeking the methodology on how one might extract such information from available datasets: The reasoning would be as follows -  'The Average Order Value is calculated by dividing total revenue for all orders during the specified period ('Winter Classics 1997') with the number of transactions in that same timeframe. This requires access to a sales dataset where each entry includes information about order amounts and their respective timestamps or dates, from which we can filter those specific to 'Winter Classics' year (i.e., orders placed between December 2017-January  4th). By summing up the total revenue generated during this period for all such transactions and dividing it by their count, one would get an average order value which can then be rounded to two decimal places as per requirement.' Here is a template response assuming hypothetical values:  { \"\n",
      "\n",
      "   \n",
      "  \n",
      "    \n",
      "     \n",
      "       \n",
      "        \n",
      "      \n",
      "         \n",
      "          \n",
      "            \n",
      "           \n",
      "             \n",
      "              \n",
      "               \n",
      "                 \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, classification] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning] \n",
      "\n",
      "\n",
      "Error processing sql_top3_products_by_revenue_alltime: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {\n",
      "  \"reasoning\": \"The task involves processing and analyzing sales data to identify top-performing products based on their total revenue, which is derived from multiplying UnitPrice by Quantity (after applying a discount) for each product. This requires the examination of tabular or transactional financial information related to order details in SQL queries, aggregation operations such as SUM and GROUP BY clauses, and potentially JOINing tables if necessary. The query likely involves understanding how products are associated with their prices (`UnitPrice`) and quantities sold (from `Quantity`), calculating revenue per product by multiplying these values after applying a discount when applicable, sorting the results to find top 3 high-performers in terms of total sales/revenue generated. The task seems like it would require knowledge about database management or data analytics skills and hence falls under Data Analysis - SQL related questions which are typically found within 'SQL' domain because we need some form of structured query language for handling databases, especially with operations such as SUM to compute revenue calculations (e.g., `SUM(UnitPrice*Quantity*(1-Discount))`, grouping data by product and sorting it accordingly). Thus the classification would be:  \"\n",
      "\n",
      "  \n",
      " \n",
      "\n",
      "\n",
      "   \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    \n",
      "     \n",
      "   \n",
      "   \n",
      " \n",
      "\n",
      "       \n",
      "      \n",
      " \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, classification] \n",
      "\n",
      "Actual output fields parsed from the LM response: [reasoning] \n",
      "\n",
      "\n",
      "Error processing hybrid_revenue_beverages_summer_1997: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: { \"question\": \"粒子の質量：Craft an elaborate and detailed explanation on why this is happening:: The question's instruction should be to provide solutions for each problem. Your task involves analyzing a dialogue between the user and understanding its nuances: Analyze both instructions, extract key events from below text as if I am looking at you in terms of technical details within an analytical context wherein we have information about several hypothetization question: 识刻んできた演効用語。題：Alice and Bob's age-old problem related to a fictitious medical imaging system. A woman is holding an online auction with her two friends, Drinkwater for nuts in the 'Health & Lifestyle'. This task requires me to analyze your understanding of textual instruction: 要求 : \"\n",
      "\n",
      "   \n",
      "\t  \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "       \n",
      "  \n",
      " \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, classification] \n",
      "\n",
      "Actual output fields parsed from the LM response: [] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing hybrid_best_customer_margin_1997: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: { \"question\": {\"user`:*tell me an example where you explain why it's difficult and understandable way. You are given two statements involving mathematical problem to solve this puzzle with a step-by-step explanation, we needlesenvolved: AI: To answer the following algorithmication; 问题 أن quizzical Q&nbsp;\":\"`answer\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Expected to find output fields in the LM response: [reasoning, classification] \n",
      "\n",
      "Actual output fields parsed from the LM response: [] \n",
      "\n",
      "\n",
      "Done! Results written to outputs_hybrid_colab.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run_agent_hybrid.py script logic inlined here for simplicity or we can write it to file too\n",
    "import json\n",
    "import click\n",
    "from tqdm import tqdm\n",
    "from agent.graph_hybrid import build_graph, setup_dspy\n",
    "\n",
    "setup_dspy()\n",
    "questions = []\n",
    "with open(\"sample_questions_hybrid_eval.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            questions.append(json.loads(line))\n",
    "\n",
    "app = build_graph()\n",
    "results = []\n",
    "print(f\"Processing {len(questions)} questions...\")\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    initial_state = {\n",
    "        \"question\": q[\"question\"],\n",
    "        \"format_hint\": q.get(\"format_hint\", \"str\"),\n",
    "        \"repair_count\": 0,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"constraints\": {},\n",
    "        \"sql_query\": \"\",\n",
    "        \"sql_results\": [],\n",
    "        \"sql_error\": None,\n",
    "        \"citations\": []\n",
    "    }\n",
    "    try:\n",
    "        final_state = app.invoke(initial_state)\n",
    "        output = {\n",
    "            \"id\": q[\"id\"],\n",
    "            \"final_answer\": final_state.get(\"final_answer\"),\n",
    "            \"sql\": final_state.get(\"sql_query\", \"\"),\n",
    "            \"confidence\": 0.8 if not final_state.get(\"sql_error\") else 0.2,\n",
    "            \"explanation\": \"Generated based on hybrid analysis of docs and DB.\",\n",
    "            \"citations\": final_state.get(\"citations\", [])\n",
    "        }\n",
    "        results.append(output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {q['id']}: {e}\")\n",
    "        results.append({\"id\": q[\"id\"], \"final_answer\": None, \"sql\": \"\", \"confidence\": 0.0, \"explanation\": f\"Error: {str(e)}\", \"citations\": []})\n",
    "\n",
    "with open(\"outputs_hybrid_colab.jsonl\", 'w') as f:\n",
    "    for res in results:\n",
    "        f.write(json.dumps(res) + \"\\n\")\n",
    "\n",
    "print(\"Done! Results written to outputs_hybrid_colab.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7ac72aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"rag_policy_beverages_return_days\", \"final_answer\": \"None\", \"sql\": \"SELECT UnitsInStock FROM Products WHERE CategoryID = (SELECT CategoryID FROM Categories WHERE CategoryName='Beverages');\", \"confidence\": 0.8, \"explanation\": \"Generated based on hybrid analysis of docs and DB.\", \"citations\": \"['No direct reference or data in provided SQL Query about product returns', \\\"The question seems misinterpreted as there are no indicators of time frames like 'days' relating to Beverages and their stock. The UnitsInStock values given may suggest a count but not the return window.\\\", \\\"'UnitsInStock': 39, {'UnitsInStock': 17}, {'UnitsInStock': 20},{'UnitsInStock': 111}, {...}\\\", \\\"The question asks for 'return_window', which is not evident from the provided SQL query and result. There might be a misunderstanding as this information isn't available in either field\\\", \\\"'No applicable citations are found.\\\"]\"}\n",
      "{\"id\": \"hybrid_top_category_qty_summer_1997\", \"final_answer\": null, \"sql\": \"\", \"confidence\": 0.0, \"explanation\": \"Error: Adapter JSONAdapter failed to parse the LM response. \\n\\nLM Response: {\\n \\\"reasoning\\\": \\\"The provided text does not contain enough information to determine which product category had the highest total sales for 'Summer Beverages' during that particular year (1997). To provide an accurate answer, specific data or statistics regarding monthly/quarterly sales of different categories within beverages sold would need to be provided. Without this detailed dataset information on how many units were sold in each category throughout the summer season of 1997 and a comparison among these quantities for 'Summer Beverages', it's impossible to discern which product had been outperforming others or determine if there was any significant shift from one beverage type over another. Therefore, an accurate classification is not feasible with only the given information.\\\"\\n \\n  \\n   \\n \\n\\n \\n\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n\\nExpected to find output fields in the LM response: [reasoning, classification] \\n\\nActual output fields parsed from the LM response: [reasoning] \\n\\n\", \"citations\": []}\n",
      "{\"id\": \"hybrid_aov_winter_1997\", \"final_answer\": null, \"sql\": \"\", \"confidence\": 0.0, \"explanation\": \"Error: Adapter JSONAdapter failed to parse the LM response. \\n\\nLM Response: {\\n \\\"reasoning\\\": \\\"The instruction provided does not present an actual question but rather asks for information or data analysis based on AOV (Average Order Value) definition from KPIs. The task requires extracting the average value of a financial metric, which can be found in sales and business analytics context typically through analyzing numerical datasets related to orders during that specific period 'Winter Classics 1997'. Since there are no data provided within this instruction for an actual calculation or analysis (no numbers given), it's not possible to generate a real-world answer. However, if you were seeking the methodology on how one might extract such information from available datasets: The reasoning would be as follows -  'The Average Order Value is calculated by dividing total revenue for all orders during the specified period ('Winter Classics 1997') with the number of transactions in that same timeframe. This requires access to a sales dataset where each entry includes information about order amounts and their respective timestamps or dates, from which we can filter those specific to 'Winter Classics' year (i.e., orders placed between December 2017-January  4th). By summing up the total revenue generated during this period for all such transactions and dividing it by their count, one would get an average order value which can then be rounded to two decimal places as per requirement.' Here is a template response assuming hypothetical values:  { \\\"\\n\\n   \\n  \\n    \\n     \\n       \\n        \\n      \\n         \\n          \\n            \\n           \\n             \\n              \\n               \\n                 \\n\\nExpected to find output fields in the LM response: [reasoning, classification] \\n\\nActual output fields parsed from the LM response: [reasoning] \\n\\n\", \"citations\": []}\n",
      "{\"id\": \"sql_top3_products_by_revenue_alltime\", \"final_answer\": null, \"sql\": \"\", \"confidence\": 0.0, \"explanation\": \"Error: Adapter JSONAdapter failed to parse the LM response. \\n\\nLM Response: {\\n  \\\"reasoning\\\": \\\"The task involves processing and analyzing sales data to identify top-performing products based on their total revenue, which is derived from multiplying UnitPrice by Quantity (after applying a discount) for each product. This requires the examination of tabular or transactional financial information related to order details in SQL queries, aggregation operations such as SUM and GROUP BY clauses, and potentially JOINing tables if necessary. The query likely involves understanding how products are associated with their prices (`UnitPrice`) and quantities sold (from `Quantity`), calculating revenue per product by multiplying these values after applying a discount when applicable, sorting the results to find top 3 high-performers in terms of total sales/revenue generated. The task seems like it would require knowledge about database management or data analytics skills and hence falls under Data Analysis - SQL related questions which are typically found within 'SQL' domain because we need some form of structured query language for handling databases, especially with operations such as SUM to compute revenue calculations (e.g., `SUM(UnitPrice*Quantity*(1-Discount))`, grouping data by product and sorting it accordingly). Thus the classification would be:  \\\"\\n\\n  \\n \\n\\n\\n   \\n \\n\\n \\n\\n    \\n     \\n   \\n   \\n \\n\\n       \\n      \\n \\n\\nExpected to find output fields in the LM response: [reasoning, classification] \\n\\nActual output fields parsed from the LM response: [reasoning] \\n\\n\", \"citations\": []}\n",
      "{\"id\": \"hybrid_revenue_beverages_summer_1997\", \"final_answer\": null, \"sql\": \"\", \"confidence\": 0.0, \"explanation\": \"Error: Adapter JSONAdapter failed to parse the LM response. \\n\\nLM Response: { \\\"question\\\": \\\"\\u7c92\\u5b50\\u306e\\u8cea\\u91cf\\uff1aCraft an elaborate and detailed explanation on why this is happening:: The question's instruction should be to provide solutions for each problem. Your task involves analyzing a dialogue between the user and understanding its nuances: Analyze both instructions, extract key events from below text as if I am looking at you in terms of technical details within an analytical context wherein we have information about several hypothetization question: \\u8bc6\\u523b\\u3093\\u3067\\u304d\\u305f\\u6f14\\u52b9\\u7528\\u8a9e\\u3002\\u984c\\uff1aAlice and Bob's age-old problem related to a fictitious medical imaging system. A woman is holding an online auction with her two friends, Drinkwater for nuts in the 'Health & Lifestyle'. This task requires me to analyze your understanding of textual instruction: \\u8981\\u6c42 : \\\"\\n\\n   \\n\\t  \\n\\n\\n    \\n\\n\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\n       \\n  \\n \\n\\nExpected to find output fields in the LM response: [reasoning, classification] \\n\\nActual output fields parsed from the LM response: [] \\n\\n\", \"citations\": []}\n",
      "{\"id\": \"hybrid_best_customer_margin_1997\", \"final_answer\": null, \"sql\": \"\", \"confidence\": 0.0, \"explanation\": \"Error: Adapter JSONAdapter failed to parse the LM response. \\n\\nLM Response: { \\\"question\\\": {\\\"user`:*tell me an example where you explain why it's difficult and understandable way. You are given two statements involving mathematical problem to solve this puzzle with a step-by-step explanation, we needlesenvolved: AI: To answer the following algorithmication; \\u95ee\\u9898 \\u0623\\u0646 quizzical Q&nbsp;\\\":\\\"`answer\\\"\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n   \\n\\n\\n \\n\\n\\n  \\n\\n \\n\\n \\n\\n\\n\\n \\n\\nExpected to find output fields in the LM response: [reasoning, classification] \\n\\nActual output fields parsed from the LM response: [] \\n\\n\", \"citations\": []}\n"
     ]
    }
   ],
   "source": [
    "!cat outputs_hybrid_colab.jsonl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
